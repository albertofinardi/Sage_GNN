\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[backend=biber,style=numeric,sorting=nyt]{biblatex}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

\addbibresource{references.bib}

% Custom commands
\newcommand{\agg}{\mathrm{AGG}}
\newcommand{\concat}{\mathrm{CONCAT}}
\newcommand{\relu}{\mathrm{ReLU}}

% Document metadata
\title{\textbf{Applying GraphSAGE to Large-Scale Node Classification}\\[0.5em]
\large Programming Machine Learning Models for HPC\\
Final Report}
\author{Thomas Gantz \and Alberto Finardi \and Tommaso Crippa \and Jan Marxen}
\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This report documents our study and application of GraphSAGE (Graph SAmple and aggreGatE)~\cite{graphsage}, an inductive framework for learning node embeddings on large-scale graphs. We provide background on graph neural networks and the message passing paradigm, then describe how GraphSAGE addresses the scalability limitations of earlier approaches by learning aggregator functions rather than fixed node embeddings. We apply GraphSAGE to the ogbn-products benchmark dataset for node classification, detailing our implementation choices, hyperparameter tuning process, and experimental results.
\end{abstract}

%=======================================================================
\section{Introduction}
\label{sec:introduction}
%=======================================================================

Graph-structured data appears in many domains, including social networks, citation graphs, biological networks, and recommendation systems. A central challenge is learning node representations (embeddings) that capture both node features and structural context.

\paragraph{Graph Prediction Tasks.}
Machine learning on graphs encompasses node-level prediction (classifying individual nodes), edge-level prediction (link prediction or edge classification), and graph-level prediction (predicting properties of entire graphs). This report focuses on \textbf{node classification}, the task addressed by GraphSAGE.

\paragraph{Graph Neural Networks.}
A Graph Neural Network (GNN) is a learnable transformation that updates node features using neural networks, respects graph structure by aggregating neighbor information, and is permutation invariant. GNNs build node representations through iterative \textbf{message passing}: at each layer, nodes gather neighbor embeddings, aggregate them with a permutation-invariant function, and apply a learned transformation.

\paragraph{Limitations of Prior Methods.}
Before GraphSAGE, methods like DeepWalk and node2vec~\cite{deepwalk,node2vec} learned embeddings via random walks but were \textit{transductive} - adding new nodes required retraining. GCNs~\cite{gcn} use message passing but operate on the full adjacency matrix, limiting scalability. The fundamental problem: \textbf{no method provided a parametric function to generate embeddings for unseen nodes.}

\paragraph{The GraphSAGE Insight.}
GraphSAGE's innovation is to learn a \textbf{function} that generates embeddings rather than learning fixed embeddings per node. By learning aggregator functions that sample and combine neighborhood features, GraphSAGE becomes \textbf{inductive}: the model generalizes to new nodes or graphs without retraining.

%=======================================================================
\section{GraphSAGE Framework}
\label{sec:graphsage-framework}
%=======================================================================

GraphSAGE (Graph SAmple and aggreGatE) implements inductive learning through neighbor sampling and learned aggregation. At each layer $k$, node $v$ updates its representation by sampling neighbors, aggregating their embeddings, concatenating with its own embedding, and applying a learned transformation with non-linearity.

\subsection{Neighbor Sampling}
\label{subsec:sampling}

For scalability, GraphSAGE uniformly samples a fixed-size set of neighbors at each layer rather than using all neighbors. For a $K$-layer model with sample sizes $S_1, \ldots, S_K$, per-node complexity is $O(\prod_{k=1}^{K} S_k)$, independent of actual node degrees.

\subsection{Aggregator Functions}
\label{subsec:aggregators}

GraphSAGE supports several aggregators: \textbf{mean} (element-wise average), \textbf{LSTM} (sequential processing of a random neighbor permutation), and \textbf{pooling} (neural network followed by max-pooling). The mean aggregator is simplest and often effective; pooling provides more expressiveness.

%=======================================================================
\section{Implementation}
\label{sec:implementation}
%=======================================================================

\subsection{Dataset and Challenge}
\label{subsec:dataset}

We evaluate GraphSAGE on the \textbf{ogbn-products} dataset~\cite{ogb}, an Amazon co-purchase network with 2.4M nodes, 61M edges, and 47 classes (8\% train, 2\% validation, 90\% test split). The scale makes full-batch training infeasible, necessitating \textbf{mini-batch training with neighbor sampling}.

\subsection{Sampling Strategy}
\label{subsec:sampling-strategy}

We employ two complementary strategies: (1) \textbf{mini-batch sampling} selects fixed-size subsets of seed nodes per iteration (e.g., 128), and (2) \textbf{neighbor sampling} recursively samples neighbors at each layer with fanout $[S_1, \ldots, S_k]$. For fanout $[15, 10, 10, 10, 10]$, each seed expands to ${\sim}150{,}000$ neighbors, yielding ${\sim}19.2$M nodes per batch (128 seeds). This subgraph expansion dominates memory (30--40 GB) versus model parameters (${\sim}8$ MB).

\subsection{Infrastructure and Software}
\label{subsec:infrastructure}

Experiments run on \textbf{MeluXina} supercomputer with 4$\times$ A100 GPUs (40 GB) per node and HDR200 InfiniBand interconnect. We use \textbf{PyTorch 2.1.2} (CUDA 12.1, cuDNN 8) with \textbf{PyTorch Geometric} for neighbor sampling (\texttt{NeighborLoader}) and GraphSAGE layers (\texttt{SAGEConv}). The pipeline is containerized via \textbf{Apptainer/Singularity} for reproducibility, bundling PyG, OGB, and compiled CUDA extensions.

\subsection{Distributed Training}
\label{subsec:ddp}

To overcome single-GPU memory limits, we use \textbf{PyTorch DDP} for multi-GPU training. The model itself is small (${\sim}400$K parameters, ${\sim}8$ MB) and requires no model parallelism or partitioning - the memory bottleneck is the sampled subgraphs (30--40 GB per batch), not model size. Thus, \textbf{data parallelism} is the appropriate strategy, making DDP ideal and rendering frameworks like DeepSpeed unnecessary. SLURM launches independent ranks per GPU, each with its own model replica and \texttt{NeighborLoader} on partitioned training data. Processes communicate via \textbf{NCCL} (NVLink intra-node, InfiniBand inter-node). Each rank computes forward/backward passes independently; DDP automatically all-reduces gradients for synchronized parameter updates. This achieves 3.73$\times$ speedup on 4 GPUs (93\% efficiency) and ${\sim}7\times$ on 8 GPUs (89\% efficiency).

\subsection{Model Architecture}
\label{subsec:architecture}

Our 5-layer GraphSAGE model uses \textbf{SAGEConv} (mean aggregation) + \textbf{LayerNorm} + \textbf{ReLU} + \textbf{Dropout} (0.5) per layer, with a final linear classifier. Architecture: 100-dim input (ogbn-products features) $\to$ 256-dim hidden $\to$ 47-dim output (${\sim}400$K parameters total).

%=======================================================================
\section{Hyperparameters}
\label{sec:hyperparameters}
%=======================================================================

\subsection{Hyperparameter Tuning Methodology}
\label{subsec:tuning-methodology}

To systematically explore the hyperparameter space and identify optimal configurations for large-scale GraphSAGE training, we developed a scalable and reproducible benchmarking pipeline. Our approach leverages three key components:

\paragraph{Apptainer Containers.}
All experiments run within identical containerized environments to ensure reproducibility across different cluster nodes and eliminate dependency variability.

\paragraph{SLURM Job Arrays.}
We encode hyperparameter configurations in CSV files and use SLURM job arrays to execute multiple training runs in parallel. Each job array task reads a specific configuration row, trains the model, and logs results to a shared output CSV.

\paragraph{Automated Result Analysis.}
Post-training, we use Python scripts to aggregate results, compute statistics, and generate comparative plots. Results are locked to prevent concurrent write conflicts.

\subsection{Hyperparameter Space}
\label{subsec:hyperparameter-space}

We explored three primary dimensions: mini-batch sampling, neighbor sampling, and GPU scaling. Table~\ref{tab:hyperparameters} summarizes the tested and fixed hyperparameters.

\begin{table}[h]
\centering
\caption{Hyperparameter configurations tested and fixed values}
\label{tab:hyperparameters}
\begin{tabular}{@{} l l l @{}}
\toprule
\textbf{Parameter} & \textbf{Default Value} & \textbf{Range Tested} \\
\midrule
\multicolumn{3}{l}{\textit{Tested Parameters}} \\
Batch size & 128 & 16, 32, 64, 128, 160, 192, 256, 512 \\
Gradient accumulation steps & 5 & 1, 2, 3, 4, 5 \\
Neighbor fanout & [15,10,10,10,10] & 16 configurations \\
GPU count & 4 & 1, 2, 4, 8 \\
\midrule
\multicolumn{3}{l}{\textit{Fixed Parameters}} \\
Number of layers & 5 &  -  \\
Hidden dimensions & 256 &  -  \\
Learning rate & 0.003 &  -  \\
Epochs & 150 &  -  \\
Early stopping patience & 10 &  -  \\
Dropout rate & 0.5 &  -  \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Effective Batch Size.}
The effective batch size is computed as:
\[
\text{Effective Batch Size} = \text{batch\_size} \times \text{accum\_steps} \times \text{world\_size}
\]
where \texttt{world\_size} is the number of GPUs. For example, with batch size 128, accumulation steps 5, and 4 GPUs, the effective batch size is $128 \times 5 \times 4 = 2560$.

\subsection{Configurations Tested}
\label{subsec:configs-tested}

In total, we evaluated \textbf{36 hyperparameter configurations}:
\begin{itemize}
    \item \textbf{Mini-batch sampling:} 16 configurations varying batch size (16--512) and gradient accumulation steps (1--5)
    \item \textbf{Neighbor sampling:} 16 fanout configurations, including uniform ([10,10,10,10,10]), decreasing ([20,15,10,5,3]), and layer-ablation strategies ([15,10,10,10,0])
    \item \textbf{GPU scaling:} 4 configurations testing 1, 2, 4, and 8 GPUs with fixed batch size 128 and fanout [15,10,10,10,10]
\end{itemize}

Each configuration was trained for up to 150 epochs with early stopping (patience 10) based on validation accuracy. Training time ranged from 7 hours to over 19 hours depending on configuration.

%=======================================================================
\section{Results}
\label{sec:results}
%=======================================================================

\subsection{Experimental Setup}

All benchmarking experiments were conducted on the MeluXina supercomputer (Luxembourg National Supercomputer) using the Accelerator Module with 4x NVIDIA A100 GPUs (40GB each) per node and HDR200 InfiniBand interconnect (400 Gb/s aggregate bandwidth). We evaluate three key dimensions: mini-batch sampling, neighbor sampling strategies, and GPU scaling.

\subsection{Mini-batch Sampling Benchmarking}

Mini-batch sampling controls the trade-off between training speed and GPU memory utilization. Figure~\ref{fig:minibatch-results} presents the impact of batch size on training time and memory consumption.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/time_per_epoch_vs_batch_size.png}
        \label{fig:batch-time}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/peak_memory_vs_batch_size.png}
        \label{fig:batch-memory}
    \end{subfigure}
    \caption{Mini-batch Sampling Analysis}
    \label{fig:minibatch-results}
\end{figure}

\textbf{Key Finding:} Batch sizes 128--160 provide optimal balance between memory utilization and speed of training.

\subsection{Neighbor Sampling Benchmarking}

Neighbor sampling strategy critically impacts both accuracy and computational cost. Figure~\ref{fig:neighbor-results} illustrates the trade-offs between accuracy, training time, and memory consumption across various fanout configurations.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/neighbor_efficiency_time_vs_accuracy.png}
        \caption{Plot showing accuracy-time trade-off. Upper-left region indicates efficient configurations.}
        \label{fig:neighbor-acc-time}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/neighbor_memory_vs_fanout.png}
        \caption{Memory consumption scales rapidly with total fanout.}
        \label{fig:neighbor-memory}
    \end{subfigure}
    \caption{Neighbor Sampling Trade-offs}
    \label{fig:neighbor-results}
\end{figure}

\textbf{Top Configurations (by Test Accuracy):}
\begin{center}
\small
\begin{tabular}{llll}
\toprule
\textbf{Rank} & \textbf{Fanout} & \textbf{Test Acc} & \textbf{Time (h)} \\
\midrule
1 & [15, 15, 15, 5, 5] & 0.8121 & 14.56 \\
2 & [15, 10, 10, 10, 0] & 0.8114 & 7.22 \\
\textbf{3} & \textbf{[15, 10, 10, 10, 10]} & \textbf{0.8096} & \textbf{19.53} \\
4 & [10, 10, 10, 10, 10] & 0.8088 & 16.46 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Finding:} Configuration [15,10,10,10,0] achieves competitive accuracy (81.14\%) in only 7.22 hours, 2.7x faster than the baseline. This suggests the final layer benefits from reduced neighbor sampling, as deeper aggregations capture sufficient context.

\subsection{GPU Scaling Benchmarking}

Distributed training via PyTorch DDP enables efficient multi-GPU utilization. We evaluate scaling from 1 to 8 GPUs (4 GPUs per node, 2 nodes).

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/scaling_time_per_epoch.png}
        \caption{Strong scaling up to 4 GPUs (93\% efficiency). 8 GPUs across 2 nodes: 89\% efficiency.}
        \label{fig:scaling-time}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../plots/scaling_train_loss.png}
        \caption{Training loss increases with GPU count due to larger effective batch sizes.}
        \label{fig:scaling-loss}
    \end{subfigure}
    \caption{Distributed Training Analysis}
    \label{fig:scaling-results}
\end{figure}

\textbf{Key Finding:} Strong intra-node scaling achieves 3.73x speedup (4 GPUs, 93\% efficiency). Inter-node scaling to 8 GPUs yields $\sim$7x speedup (89\% efficiency), showing acceptable degradation. The trade-off between training loss and wall-clock time is favorable: modest loss increase for 7x speedup justifies distributed training.

%=======================================================================
\section{Discussion}
\label{sec:discussion}
%=======================================================================

\subsection{Bottlenecks Discovered}

\textbf{Memory Wall:} The primary bottleneck is not computation but memory. Neighbor sampling causes exponential subgraph expansion: fanout [15,10,10,10,10] expands each seed node to 150k neighbors. With batch size 128, this yields 19.2M nodes per batch, requiring most of the GPU memory for subgraph storage and intermediate activations. Model parameters account for only $\sim$8MB.

\textbf{Inter-node Communication Overhead:} Gradient synchronization via InfiniBand incurs costs. Intra-node scaling (1→4 GPUs) maintains 93\% efficiency; inter-node scaling (4→8 GPUs) drops to 89\%, representing a 4\% efficiency loss. This motivates careful consideration of multi-node training.

\textbf{Loss Convergence Trade-off:} Larger effective batch sizes (product of batch size and GPU count) lead to higher training loss. Effective batch size becomes 512 (4 GPUs), 1024 (8 GPUs), etc., contributing to wider minima and reduced gradient noise.

\subsection{What Worked Well}

\textbf{DDP Implementation:} PyTorch's DistributedDataParallel seamlessly handles gradient synchronization with minimal code changes from single-GPU baseline. Automatic all-reduce operations ensure consistent parameter updates across ranks.

\textbf{Strong Intra-node Scaling:} The system scales nearly linearly up to 4 GPUs on a single node, with PCIe/NVLink providing sufficient bandwidth. This validates the approach for datasets fitting within node memory.

\textbf{Containerization:} Apptainer/Singularity ensured reproducibility across HPC environments, isolating complex PyTorch Geometric dependencies and avoiding version conflicts.

\subsection{Key Trade-offs}

\begin{itemize}
    \item \textbf{Batch Size vs. Memory:} Batch 128--160 balances throughput and memory efficiency; further growth hits hard limits.
    \item \textbf{Receptive Field vs. Fanout:} Higher fanouts capture more context but cause exponential memory growth.
    \item \textbf{Speedup vs. Convergence:} 8 GPU training achieves 7x wall-clock speedup at the cost of slightly higher training loss. Test accuracy remains competitive, justifying the trade-off.

\end{itemize}

\subsection{Lessons Learned}

\begin{enumerate}

    \item \textbf{Accuracy vs. Performance:} [15,10,10,10,0] shows better accuracy with 2.7x speedup compared to baseline [15,10,10,10,10], showing that the 5th layer can be removed without significant accuracy loss.
    \item \textbf{Profile before optimizing:} Memory emerged as the true bottleneck, not computation. Early profiling revealed this, guiding optimization focus.
    \item \textbf{No free lunch:} Every optimization involves trade-offs. Larger batches speed training but hurt convergence. Distributed training provides speedup at communication cost.
    \item \textbf{Reproducibility is paramount:} Containerization, fixed seeds, and detailed logging are essential for scientific HPC work. Variability in system state can obscure true performance characteristics.
    
\end{enumerate}

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

This study demonstrates that GraphSAGE, when properly tuned and deployed on HPC infrastructure, scales effectively to large-scale node classification tasks. Key findings include: (1) batch sizes 128--160 offer optimal memory-speed trade-offs; (2) neighbor sampling strategy [15,10,10,10,0] achieves baseline accuracy with 2.7x speedup; (3) strong intra-node scaling validates multi-GPU deployment on a single node; (4) memory is the critical bottleneck, not computation. Future work could explore gradient compression, asynchronous training, and adaptive neighbor sampling to further improve efficiency.



%=======================================================================
\printbibliography
%=======================================================================

\end{document}

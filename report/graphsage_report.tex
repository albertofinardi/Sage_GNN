\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[backend=biber,style=numeric,sorting=nyt]{biblatex}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

\addbibresource{references.bib}

% Custom commands
\newcommand{\agg}{\mathrm{AGG}}
\newcommand{\concat}{\mathrm{CONCAT}}
\newcommand{\relu}{\mathrm{ReLU}}

% Document metadata
\title{\textbf{Applying GraphSAGE to Large-Scale Node Classification}\\[0.5em]
\large Programming Machine Learning Models for HPC\\
Final Report}
\author{Thomas Gantz \and Alberto Finardi \and Tommaso Crippa \and Jan Marxen}
\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This report documents our study and application of GraphSAGE (Graph SAmple and aggreGatE)~\cite{graphsage}, an inductive framework for learning node embeddings on large-scale graphs. We provide background on graph neural networks and the message passing paradigm, then describe how GraphSAGE addresses the scalability limitations of earlier approaches by learning aggregator functions rather than fixed node embeddings. We apply GraphSAGE to the ogbn-products benchmark dataset for node classification, detailing our implementation choices, hyperparameter tuning process, and experimental results.
\end{abstract}

\tableofcontents
\newpage

%=======================================================================
\section{Introduction}
\label{sec:introduction}
%=======================================================================

Graph-structured data appears in many domains, including social networks, citation graphs, biological networks, and recommendation systems. A central challenge is learning node representations (embeddings) that capture both node features and structural context.

\paragraph{Graph Prediction Tasks.}
Machine learning on graphs encompasses node-level prediction (classifying individual nodes), edge-level prediction (link prediction or edge classification), and graph-level prediction (predicting properties of entire graphs). This report focuses on \textbf{node classification}, the task addressed by GraphSAGE.

\paragraph{Graph Neural Networks.}
A Graph Neural Network (GNN) is a learnable transformation that updates node features using neural networks, respects graph structure by aggregating neighbor information, and is permutation invariant. GNNs build node representations through iterative \textbf{message passing}: at each layer, nodes gather neighbor embeddings, aggregate them with a permutation-invariant function, and apply a learned transformation.

\paragraph{Limitations of Prior Methods.}
Before GraphSAGE, methods like DeepWalk and node2vec~\cite{deepwalk,node2vec} learned embeddings via random walks but were \textit{transductive}---adding new nodes required retraining. GCNs~\cite{gcn} use message passing but operate on the full adjacency matrix, limiting scalability. The fundamental problem: \textbf{no method provided a parametric function to generate embeddings for unseen nodes.}

\paragraph{The GraphSAGE Insight.}
GraphSAGE's innovation is to learn a \textbf{function} that generates embeddings rather than learning fixed embeddings per node. By learning aggregator functions that sample and combine neighborhood features, GraphSAGE becomes \textbf{inductive}: the model generalizes to new nodes or graphs without retraining.

%=======================================================================
\section{GraphSAGE Framework}
\label{sec:graphsage-framework}
%=======================================================================

GraphSAGE (Graph SAmple and aggreGatE) implements inductive learning through neighbor sampling and learned aggregation. At each layer $k$, node $v$ updates its representation by sampling neighbors, aggregating their embeddings, concatenating with its own embedding, and applying a learned transformation with non-linearity.

\subsection{Neighbor Sampling}
\label{subsec:sampling}

For scalability, GraphSAGE uniformly samples a fixed-size set of neighbors at each layer rather than using all neighbors. For a $K$-layer model with sample sizes $S_1, \ldots, S_K$, per-node complexity is $O(\prod_{k=1}^{K} S_k)$, independent of actual node degrees.

\subsection{Aggregator Functions}
\label{subsec:aggregators}

GraphSAGE supports several aggregators: \textbf{mean} (element-wise average), \textbf{LSTM} (sequential processing of a random neighbor permutation), and \textbf{pooling} (neural network followed by max-pooling). The mean aggregator is simplest and often effective; pooling provides more expressiveness.

%=======================================================================
\section{Implementation}
\label{sec:implementation}
%=======================================================================

\textit{Implementation details will be provided here, including our PyTorch/DGL code structure, data loading pipeline, and distributed training setup.}

%=======================================================================
\section{Hyperparameters}
\label{sec:hyperparameters}
%=======================================================================

\textit{Hyperparameter tuning methodology and final configurations will be documented here, including learning rate schedules, hidden dimensions, number of layers, sample sizes, dropout rates, and batch sizes.}

%=======================================================================
\section{Results}
\label{sec:results}
%=======================================================================

\textit{Experimental results on the ogbn-products dataset will be presented here, including accuracy metrics, training curves, and comparisons with baseline methods.}

%=======================================================================
\section{Conclusion}
\label{sec:conclusion}
%=======================================================================

\textit{Summary of findings and conclusions will be provided here.}

%=======================================================================
\printbibliography
%=======================================================================

\end{document}
